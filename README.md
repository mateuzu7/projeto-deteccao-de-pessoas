# projeto-deteccao-de-pessoas
Sistema de IA para detec√ß√£o de pessoas na sala de estudos do Campus da UFC de Itapaj√©

# Sistema de Detec√ß√£o de Pessoas em Ambientes do Campus

Este projeto apresenta uma solu√ß√£o de Vis√£o Computacional baseada em Redes Neurais Convolucionais (CNN) para a detec√ß√£o autom√°tica de pessoas. O sistema foi desenvolvido como requisito final da disciplina de Intelig√™ncia Artificial (2025.2), utilizando o framework **Detectron2**.

## üéØ Objetivo e Aplica√ß√£o em Seguran√ßa da Informa√ß√£o

O objetivo principal √© monitorar ambientes reais do Campus (laborat√≥rios, corredores e salas de aula) para apoiar a seguran√ßa patrimonial e f√≠sica.

**Aplica√ß√µes em Seguran√ßa:**
1.  **Monitoramento de Per√≠metro:** Detec√ß√£o de intrus√£o em √°reas restritas (ex: laborat√≥rios de servidores) fora do hor√°rio comercial.
2.  **An√°lise de Ocupa√ß√£o:** Controle de lota√ß√£o em tempo real para conformidade com normas de seguran√ßa (evacua√ß√£o) e preven√ß√£o de aglomera√ß√µes.
3.  **Auditoria de Acesso:** Registro visual automatizado de entradas e sa√≠das sem necessidade de interven√ß√£o humana constante.

## üõ†Ô∏è Tecnologias Utilizadas

* **Linguagem:** Python
* **Framework:** Detectron2 (Facebook AI Research)
* **Modelo Base:** Mask R-CNN (ResNet-50-FPN) pr√©-treinado no dataset COCO.
* **T√©cnica:** Transfer Learning (Fine-Tuning) para a classe `pessoas`.
* **Rotulagem:** Roboflow (Formato COCO JSON).

## üìä Metodologia

1.  **Coleta de Dados:** Foram capturadas ~94 imagens em ambientes do Campus, variando ilumina√ß√£o e √¢ngulos.
2.  **Rotulagem:** Anota√ß√£o manual utilizando Pol√≠gonos (Segmenta√ß√£o de Inst√¢ncias) para delimitar precisamente o contorno das pessoas. Esta abordagem permite que o modelo aprenda n√£o apenas a localiza√ß√£o (Bounding Box), mas a forma exata dos indiv√≠duos nos ambientes do Campus.
3.  **Treinamento:**
    * **Itera√ß√µes:** 1000
    * **Learning Rate:** 0.00025
    * **Batch Size:** 2
    * **Num Classes:** 2 (Mapeamento ajustado para compatibilidade com Roboflow).

## üìà Resultados e M√©tricas

O modelo alcan√ßou resultados expressivos para o dataset de teste:

| M√©trica | Valor | Interpreta√ß√£o |
| :--- | :--- | :--- |
| **mAP (IoU=0.50:0.95)** | **67.1%** | Alta precis√£o geral na detec√ß√£o. |
| **AP50 (IoU=0.50)** | **87.8%** | O modelo detecta corretamente a presen√ßa humana em quase 88% dos casos. |
| **AP75** | **81.4%** | Alta fidelidade no ajuste da caixa delimitadora. |

### Exemplos Visuais

**1. Detec√ß√£o em Imagem Est√°tica:**
![Exemplo de Detec√ß√£o](results/images/testenovo.png)


**2. Monitoramento em Tempo Real (Webcam):**
O sistema √© capaz de realizar infer√™ncia em v√≠deo, simulando uma c√¢mera de seguran√ßa IP.
![Webcam Demo](results/images/detect20-01)


## üöÄ Como Executar

‚ö†Ô∏è Importante:
Este projeto necessita de GPU para treinamento e infer√™ncia.
Execute exclusivamente no Google Colab com GPU ativada.

üîß 1. Configurar o Ambiente no Google Colab

---
## 3. Passo a Passo Para Execu√ß√£o
## Preparando o Ambiente e Instalando o Detectron2
Inicialmente, estando no ambiente de nuvem (Colab), altere o ambiente de execu√ß√£o para GPU. Depois, em uma c√©lula, verifique a exist√™ncia da GPU:

```python
!nvidia-smi
```

Se bem-sucedida, voc√™ ver√° algo como:
```python
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------|
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |
| N/A   58C    P0             29W /   70W |    5554MiB /  15360MiB |      0%      Default |
+-----------------------------------------------------------------------------------------+
```
Em seguida, adicione o arquivo zipado do seu dataset no formato COCO-like ao diret√≥rio /content do ambiente e execute:
```python
!unzip "PESSOA.v1-roboflow-instant-1--eval-.coco.zip"
```
Instale o Detectron2 no ambiente:
```python
!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'
```
## Configura√ß√£o do Dataset e Treinamento do Modelo
Nesta etapa, o c√≥digo realiza o registro dos datasets no formato COCO, configurando os conjuntos de treino, valida√ß√£o e teste. Em seguida, prepara o modelo Faster R-CNN usando o Detectron2, definindo par√¢metros essenciais como n√∫mero de classes, taxa de aprendizado, tamanho do batch, n√∫mero de itera√ß√µes e pesos iniciais.

O c√≥digo tamb√©m cria a pasta de sa√≠da para armazenar os resultados e executa o treinamento do modelo, ajustando os pesos para que ele aprenda a detectar pessoas nas imagens do dataset
Esta fase pode ser implementada usando `/projeto-deteccao-pessoas/training/train.py`. Lembre-se de substituir a classe "person" pelas classes espec√≠ficas do seu dataset.`
```python
#/train.py

import torch, detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

import os, cv2, random
from google.colab.patches import cv2_imshow
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor, DefaultTrainer
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.data.datasets import register_coco_instances

# Registrando os datasets (usando os nomes das pastas que o zip criou)
try:
    register_coco_instances("person_train", {}, "/content/train/_annotations.coco.json", "/content/train")
    register_coco_instances("person_valid", {}, "/content/valid/_annotations.coco.json", "/content/valid")
    register_coco_instances("person_test", {}, "/content/test/_annotations.coco.json", "/content/test")
except:
    print("Datasets j√° registrados ou erro nos caminhos.")

person_metadata = MetadataCatalog.get("person_train")
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("person_train",)
cfg.DATASETS.TEST = ("person_valid",) # Valida√ß√£o durante o treino
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")

cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.00025
cfg.SOLVER.MAX_ITER = 1000 # Quantidade boa para 94 fotos
cfg.SOLVER.STEPS = []

# CORRE√á√ÉO: Definindo 2 classes (0: objects, 1: person)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2

# Adicionando a corre√ß√£o para o formato das m√°scaras

cfg.OUTPUT_DIR = "/content/output"
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)

trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=False)
trainer.train()
```
## Infer√™ncia e Visualiza√ß√£o de Detec√ß√µes do Modelo Treinado
Aqui, o c√≥digo carrega o modelo treinado e define o limiar m√≠nimo de confian√ßa para considerar uma detec√ß√£o v√°lida. Em seguida, realiza infer√™ncia em imagens de teste, filtrando apenas a classe person para exibir as pessoas detectadas.

Os resultados s√£o visualizados graficamente, com caixas delimitadoras sobrepostas em fundo preto, destacando os objetos detectados. Tamb√©m √© poss√≠vel testar novas imagens externas, substituindo o caminho da imagem pelo da sua pr√≥pria foto.
O c√≥digo desta etapa est√° em `/projeto-deteccao-pessoas/inference/test_model.py.`
```python
#test_model.p

# Carregar o modelo que acabou de ser treinado
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # Confian√ßa m√≠nima
predictor = DefaultPredictor(cfg)

from detectron2.utils.visualizer import ColorMode

dataset_dicts = DatasetCatalog.get("person_test")
for d in random.sample(dataset_dicts, 3):
    im = cv2.imread(d["file_name"])
    outputs = predictor(im)

    # Filtramos para mostrar apenas a classe 1 (person)
    instances = outputs["instances"].to("cpu")
    mask = instances.pred_classes == 1
    person_only = instances[mask]

    v = Visualizer(im[:, :, ::-1],
                   metadata=person_metadata,
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE_BW # Fundo PB destaca a detec√ß√£o
    )
    out = v.draw_instance_predictions(person_only)
    print(f"Resultado para: {d['file_name']}")
    cv2_imshow(out.get_image()[:, :, ::-1]) 
  
import cv2
from google.colab.patches import cv2_imshow
from detectron2.utils.visualizer import Visualizer, ColorMode

# 1. Caminho da sua nova imagem
caminho_imagem_nova = "/content/WhatsApp Image 2026-01-18 at 11.06.07 PM.jpeg" # Substitua pelo nome do seu arquivo

# 2. Carregar a imagem com o OpenCV
im = cv2.imread(caminho_imagem_nova)

# 3. Fazer a predi√ß√£o (o modelo vai analisar a imagem)
outputs = predictor(im)

# 4. Filtrar para mostrar apenas a classe 1 (person)
# Isso evita que o modelo mostre a classe 0 (vazia)
instances = outputs["instances"].to("cpu")
person_only = instances[instances.pred_classes == 1]

# 5. Visualizar o resultado
v = Visualizer(im[:, :, ::-1],
               metadata=person_metadata,
               scale=0.8,
               instance_mode=ColorMode.IMAGE_BW)

out = v.draw_instance_predictions(person_only)
cv2_imshow(out.get_image()[:, :, ::-1])

```
## Avalia√ß√£o de Desempenho com M√©tricas COCO
Nesta etapa, o sistema realiza a avalia√ß√£o quantitativa do modelo utilizando o COCOEvaluator para calcular m√©tricas de desempenho sobre o conjunto de teste. Apenas as bounding boxes s√£o avaliadas, evitando problemas com segmenta√ß√£o de m√°scaras.

O modelo processa todas as imagens de teste e gera m√©tricas como Average Precision (AP) e recall, permitindo verificar a acur√°cia do detector de pessoas de forma objetiva. Os resultados s√£o exibidos no console e podem ser salvos para an√°lises posteriores.

O c√≥digo de avalia√ß√£o pode ser encontrado em `/projeto-deteccao-pessoas/results/metrics/evaluation.py`.
```python
#evaluation.py

from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader

# Avaliamos apenas BBOX (caixas) para evitar o erro de segmenta√ß√£o que voc√™ teve
evaluator = COCOEvaluator("person_test", output_dir="./output", tasks=("bbox",))
val_loader = build_detection_test_loader(cfg, "person_test")

print("--- M√âTRICAS DE DESEMPENHO ---")
results = inference_on_dataset(predictor.model, val_loader, evaluator)
print(results)
```

## Monitoramento em Tempo Real com C√¢mera
Essa etapa permite monitoramento de v√≠deo em tempo real usando a c√¢mera do dispositivo. O c√≥digo inicializa a captura, converte frames em imagens process√°veis e exibe a interface de streaming no navegador com sobreposi√ß√£o de informa√ß√µes.

Para cada frame capturado, o modelo detecta pessoas (classe person) e exibe caixas delimitadoras sobre um canvas preto, junto com um contador do n√∫mero de pessoas detectadas. O loop continua at√© o usu√°rio interromper o monitoramento, permitindo avalia√ß√£o din√¢mica do modelo em cen√°rios reais, √∫til para vigil√¢ncia e sistemas de seguran√ßa.

O c√≥digo do monitoramento por webcam est√° em `/projeto-deteccao-pessoas/inference/webcam-monitoring.py`.
```python
#webcam-monitoring.py

# --- 1. IMPORTS NECESS√ÅRIOS (Isso estava faltando) ---
from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import numpy as np
import cv2
import io
import PIL.Image
import os

# Certifique-se de que o cfg e o predictor j√° foram definidos nas c√©lulas anteriores!
# Se der erro de 'cfg not defined', rode a c√©lula de configura√ß√£o do modelo antes.

# --- 2. FUN√á√ïES DE SUPORTE PARA O V√çDEO ---
def array_to_image(a):
    res = PIL.Image.fromarray(a)
    byte_io = io.BytesIO()
    res.save(byte_io, format='PNG')
    return b64encode(byte_io.getvalue()).decode('ascii')

def video_stream():
  js = Javascript('''
    var video; var div = null; var stream; var captureCanvas; var imgElement; var labelElement;
    var pendingResolve = null; var shutdown = false;

    function removeDom() {
       if (stream) stream.getTracks().forEach(t => t.stop());
       if (video) video.remove();
       if (div) div.remove();
       video = null; div = null; stream = null; imgElement = null; captureCanvas = null; labelElement = null;
    }

    function onAnimationFrame() {
      if (!shutdown) window.requestAnimationFrame(onAnimationFrame);
      if (pendingResolve) {
        var result = "";
        if (!shutdown) {
          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);
          result = captureCanvas.toDataURL('image/jpeg', 0.8)
        }
        var lp = pendingResolve; pendingResolve = null; lp(result);
      }
    }

    async function createDom() {
      div = document.createElement('div');
      div.style.border = '2px solid red'; div.style.padding = '10px'; div.style.width = '660px'; div.style.background = '#000';
      labelElement = document.createElement('div');
      labelElement.innerText = "SISTEMA DE SEGURAN√áA ATIVO";
      labelElement.style.color = 'white'; labelElement.style.fontWeight = 'bold';
      div.appendChild(labelElement);
      video = document.createElement('video');
      video.style.display = 'block'; video.width = 640; video.height = 480;
      div.appendChild(video);
      stream = await navigator.mediaDevices.getUserMedia({video: {width: 640, height: 480}});
      video.srcObject = stream; await video.play();
      captureCanvas = document.createElement('canvas');
      captureCanvas.width = 640; captureCanvas.height = 480;
      imgElement = document.createElement('img');
      imgElement.style.position = 'absolute'; imgElement.style.top = '40px'; imgElement.style.left = '20px';
      imgElement.style.opacity = '0.8'; div.appendChild(imgElement);
      const stopBtn = document.createElement('button');
      stopBtn.textContent = "PARAR MONITORAMENTO";
      stopBtn.onclick = () => { shutdown = true; };
      div.appendChild(stopBtn);
      document.body.appendChild(div);
      window.requestAnimationFrame(onAnimationFrame);
    }

    async function stream_frame(label, imgData) {
      if (shutdown) { removeDom(); shutdown = false; return ""; }
      if (div === null) await createDom();
      if (labelElement) labelElement.innerText = label;
      if (imgData) imgElement.src = imgData;
      return new Promise((resolve) => { pendingResolve = resolve; });
    }
    ''')
  display(js)

# --- 3. LOOP PRINCIPAL ---
# Recarrega os pesos para garantir
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
predictor = DefaultPredictor(cfg)

video_stream()
label_html = 'Iniciando C√¢mera...'
bbox_img_data = ''

try:
    while True:
        img_data = eval_js('stream_frame("{}", "{}")'.format(label_html, bbox_img_data))
        if not img_data: break

        binary = b64decode(img_data.split(',')[1])
        img = cv2.imdecode(np.frombuffer(binary, np.uint8), -1)
        outputs = predictor(img)
        instances = outputs["instances"].to("cpu")

        # Filtrar apenas classe 1 (person)
        person_only = instances[instances.pred_classes == 1]

        canvas = np.zeros((480, 640, 3), dtype=np.uint8)
        v = Visualizer(canvas, metadata=person_metadata, scale=1.0)
        out = v.draw_instance_predictions(person_only)

        bbox_img_data = 'data:image/png;base64,' + array_to_image(out.get_image())
        label_html = f"SEGURAN√áA CAMPUS: {len(person_only)} PESSOA(S) DETECTADA(S)"
except Exception as e:
    print("Monitoramento finalizado.")
```

## üìÅ Estrutura do Reposit√≥rio

* `data/`: Amostras do dataset e anota√ß√µes.
* `training/`: Scripts de configura√ß√£o e treinamento (Fine-tuning).
* `inference/`: Scripts para teste em imagens e webcam.
* `results/`: Gr√°ficos de m√©tricas e evid√™ncias visuais.
* `model/`: (Link para download do modelo .pth).

---
**Autor:** Mateus Oliveira
**Disciplina:** Intelig√™ncia Artificial - 2025.2
