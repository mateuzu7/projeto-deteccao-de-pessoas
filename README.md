# projeto-deteccao-de-pessoas
Sistema de IA para detec√ß√£o de pessoas na sala de estudos do Campus da UFC de Itapaj√©

# Sistema de Detec√ß√£o de Pessoas em Ambientes do Campus

Este projeto apresenta uma solu√ß√£o de Vis√£o Computacional baseada em Redes Neurais Convolucionais (CNN) para a detec√ß√£o autom√°tica de pessoas. O sistema foi desenvolvido como requisito final da disciplina de Intelig√™ncia Artificial (2025.2), utilizando o framework **Detectron2**.

## üéØ Objetivo e Aplica√ß√£o em Seguran√ßa da Informa√ß√£o

O objetivo principal √© monitorar ambientes reais do Campus (laborat√≥rios, corredores e salas de aula) para apoiar a seguran√ßa patrimonial e f√≠sica.

**Aplica√ß√µes em Seguran√ßa:**
1.  **Monitoramento de Per√≠metro:** Detec√ß√£o de intrus√£o em √°reas restritas (ex: laborat√≥rios de servidores) fora do hor√°rio comercial.
2.  **An√°lise de Ocupa√ß√£o:** Controle de lota√ß√£o em tempo real para conformidade com normas de seguran√ßa (evacua√ß√£o) e preven√ß√£o de aglomera√ß√µes.
3.  **Auditoria de Acesso:** Registro visual automatizado de entradas e sa√≠das sem necessidade de interven√ß√£o humana constante.

## üõ†Ô∏è Tecnologias Utilizadas

* **Linguagem:** Python
* **Framework:** Detectron2 (Facebook AI Research)
* **Modelo Base:** Mask R-CNN (ResNet-50-FPN) pr√©-treinado no dataset COCO.
* **T√©cnica:** Transfer Learning (Fine-Tuning) para a classe `pessoas`.
* **Rotulagem:** Roboflow (Formato COCO JSON).

## üìä Metodologia

1.  **Coleta de Dados:** Foram capturadas ~94 imagens em ambientes do Campus, variando ilumina√ß√£o e √¢ngulos.
2.  **Rotulagem:** Anota√ß√£o manual utilizando Pol√≠gonos (Segmenta√ß√£o de Inst√¢ncias) para delimitar precisamente o contorno das pessoas. Esta abordagem permite que o modelo aprenda n√£o apenas a localiza√ß√£o (Bounding Box), mas a forma exata dos indiv√≠duos nos ambientes do Campus.
3.  **Treinamento:**
    * **Itera√ß√µes:** 1000
    * **Learning Rate:** 0.00025
    * **Batch Size:** 2
    * **Num Classes:** 2 (Mapeamento ajustado para compatibilidade com Roboflow).

## üìà Resultados e M√©tricas

O modelo alcan√ßou resultados expressivos para o dataset de teste:

| M√©trica | Valor | Interpreta√ß√£o |
| :--- | :--- | :--- |
| **mAP (IoU=0.50:0.95)** | **67.1%** | Alta precis√£o geral na detec√ß√£o. |
| **AP50 (IoU=0.50)** | **87.8%** | O modelo detecta corretamente a presen√ßa humana em quase 88% dos casos. |
| **AP75** | **81.4%** | Alta fidelidade no ajuste da caixa delimitadora. |

### Exemplos Visuais

**1. Detec√ß√£o em Imagem Est√°tica:**
![Exemplo de Detec√ß√£o](results/images/testenovo.png)


**2. Monitoramento em Tempo Real (Webcam):**
O sistema √© capaz de realizar infer√™ncia em v√≠deo, simulando uma c√¢mera de seguran√ßa IP.
![Webcam Demo](results/images/detect20-01)


## üöÄ Como Executar

‚ö†Ô∏è Importante:
Este projeto necessita de GPU para treinamento e infer√™ncia.
Execute exclusivamente no Google Colab com GPU ativada.

üîß 1. Configurar o Ambiente no Google Colab

---
```
# 3. Passo a Passo Para Execu√ß√£o

## üõ†Ô∏è Preparando o Ambiente e Instalando o Detectron2

Inicialmente, estando no ambiente de nuvem (Google Colab), altere o ambiente de execu√ß√£o para **GPU**.
Depois, verifique a exist√™ncia e o status da GPU executando a c√©lula abaixo:

```bash
!nvidia-smi

```

Se bem-sucedida, voc√™ ver√° uma tabela mostrando a GPU (ex: Tesla T4).

Em seguida, adicione o arquivo zipado do seu dataset (exportado do Roboflow) ao diret√≥rio `/content` do ambiente e execute o comando exato abaixo para descompactar (note o uso de aspas devido aos espa√ßos no nome):


```
!unzip "Detect.v1-roboflow-instant-1--eval-.coco (1).zip"

```

Instale a vers√£o est√°vel do Detectron2 compat√≠vel com o Colab:


```
!python -m pip install 'git+[https://github.com/facebookresearch/detectron2.git](https://github.com/facebookresearch/detectron2.git)'

```

* * * * *

‚öôÔ∏è Configura√ß√£o do Dataset e Treinamento do Modelo
--------------------------------------------------

Nesta etapa, o c√≥digo realiza o registro dos datasets (`train`, `valid`, `test`) no formato COCO. Em seguida, prepara o modelo **Mask R-CNN** (ResNet-50-FPN) usando o Detectron2.

Diferente do Faster R-CNN, este modelo √© capaz de segmenta√ß√£o, mas aqui estamos focando na detec√ß√£o. Definimos **2 classes** (0: objects, 1: pessoas) e configuramos os hiperpar√¢metros de treino.

Arquivo: `/projeto-deteccao-pessoas/training/train.py`


```python
import torch, detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

import os, cv2, random
from google.colab.patches import cv2_imshow
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor, DefaultTrainer
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.data.datasets import register_coco_instances

# Registrando os datasets (usando os nomes das pastas que o zip criou)
try:
    register_coco_instances("pessoas_train", {}, "/content/train/_annotations.coco.json", "/content/train")
    register_coco_instances("pessoas_valid", {}, "/content/valid/_annotations.coco.json", "/content/valid")
    register_coco_instances("pessoas_test", {}, "/content/test/_annotations.coco.json", "/content/test")
except:
    print("Datasets j√° registrados ou erro nos caminhos.")

pessoas_metadata = MetadataCatalog.get("pessoas_train")

# Configura√ß√£o do Modelo Mask R-CNN
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))

cfg.DATASETS.TRAIN = ("pessoas_train",)
cfg.DATASETS.TEST = ("pessoas_valid",) # Valida√ß√£o durante o treino
cfg.DATALOADER.NUM_WORKERS = 2

# Pesos iniciais (Transfer Learning)
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")

cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.00025
cfg.SOLVER.MAX_ITER = 1000 # Quantidade ajustada para o dataset
cfg.SOLVER.STEPS = []

# DEFINI√á√ÉO DE CLASSES: 2 (0: objects, 1: pessoas)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2

cfg.OUTPUT_DIR = "/content/output"
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)

# Iniciar Treinamento
trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=False)
trainer.train()

```

* * * * *

üîç Infer√™ncia e Visualiza√ß√£o
----------------------------

Aqui, o c√≥digo carrega os pesos treinados (`model_final.pth`) e realiza infer√™ncia em:

1.  Imagens aleat√≥rias do conjunto de teste.

2.  Uma imagem externa espec√≠fica (ex: foto do WhatsApp).

O script filtra especificamente a **classe 1 ("pessoas")**, ignorando outros objetos, e exibe o resultado com fundo preto e branco (`ColorMode.IMAGE_BW`) para destacar a detec√ß√£o.

Arquivo: `/projeto-deteccao-pessoas/inference/test_model.py`



```python
import os, cv2, random
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer, ColorMode
from google.colab.patches import cv2_imshow

# Carregar o modelo treinado
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # Confian√ßa m√≠nima
predictor = DefaultPredictor(cfg)

print("--- TESTE 1: IMAGENS ALEAT√ìRIAS ---")
dataset_dicts = DatasetCatalog.get("pessoas_test")
for d in random.sample(dataset_dicts, 3):
    im = cv2.imread(d["file_name"])
    outputs = predictor(im)

    # Filtramos para mostrar apenas a classe 1 (pessoas)
    instances = outputs["instances"].to("cpu")
    mask = instances.pred_classes == 1
    pessoas_only = instances[mask]

    v = Visualizer(im[:, :, ::-1],
                   metadata=pessoas_metadata,
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE_BW # Fundo PB destaca a detec√ß√£o
    )
    out = v.draw_instance_predictions(pessoas_only)
    print(f"Resultado para: {d['file_name']}")
    cv2_imshow(out.get_image()[:, :, ::-1])

print("\n--- TESTE 2: IMAGEM EXTERNA ---")
# Caminho da sua nova imagem
caminho_imagem_nova = "/content/WhatsApp Image 2026-01-18 at 11.06.07 PM.jpeg"

if os.path.exists(caminho_imagem_nova):
    im = cv2.imread(caminho_imagem_nova)
    outputs = predictor(im)

    # Filtrar classe 1 (pessoas)
    instances = outputs["instances"].to("cpu")
    pessoas_only = instances[instances.pred_classes == 1]

    v = Visualizer(im[:, :, ::-1],
                   metadata=pessoas_metadata,
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE_BW)

    out = v.draw_instance_predictions(pessoas_only)
    cv2_imshow(out.get_image()[:, :, ::-1])
else:
    print("Imagem externa n√£o encontrada. Verifique o caminho.")

```

* * * * *

üìä Avalia√ß√£o de Desempenho (M√©tricas COCO)
------------------------------------------

O sistema realiza a avalia√ß√£o quantitativa utilizando o `COCOEvaluator`. Configuramos `tasks=("bbox",)` para avaliar apenas as caixas delimitadoras, evitando erros relacionados √† segmenta√ß√£o (m√°scaras) caso o dataset n√£o esteja perfeitamente rotulado para tal.

Arquivo: `/projeto-deteccao-pessoas/results/metrics/evaluation.py`

```python
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader

# Avaliamos apenas BBOX (caixas) para evitar erro de segmenta√ß√£o
evaluator = COCOEvaluator("pessoas_test", output_dir="./output", tasks=("bbox",))
val_loader = build_detection_test_loader(cfg, "pessoas_test")

print("--- M√âTRICAS DE DESEMPENHO ---")
results = inference_on_dataset(predictor.model, val_loader, evaluator)
print(results)

```

* * * * *

üìπ Monitoramento em Tempo Real (Webcam)
---------------------------------------

Essa etapa injeta c√≥digo JavaScript no Colab para acessar a webcam do navegador. O Python processa cada frame, detecta a classe "pessoas" e desenha as caixas em tempo real.

Arquivo: `/projeto-deteccao-pessoas/inference/webcam_monitoring.py`



```python
# --- 1. IMPORTS NECESS√ÅRIOS ---
from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import numpy as np
import cv2
import io
import PIL.Image
import os

# (Assume que cfg e predictor j√° est√£o carregados das etapas anteriores)

# --- 2. FUN√á√ïES DE SUPORTE JS/PYTHON ---
def array_to_image(a):
    res = PIL.Image.fromarray(a)
    byte_io = io.BytesIO()
    res.save(byte_io, format='PNG')
    return b64encode(byte_io.getvalue()).decode('ascii')

def video_stream():
  js = Javascript('''
    var video; var div = null; var stream; var captureCanvas; var imgElement; var labelElement;
    var pendingResolve = null; var shutdown = false;

    function removeDom() {
       if (stream) stream.getTracks().forEach(t => t.stop());
       if (video) video.remove();
       if (div) div.remove();
       video = null; div = null; stream = null; imgElement = null; captureCanvas = null; labelElement = null;
    }

    function onAnimationFrame() {
      if (!shutdown) window.requestAnimationFrame(onAnimationFrame);
      if (pendingResolve) {
        var result = "";
        if (!shutdown) {
          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);
          result = captureCanvas.toDataURL('image/jpeg', 0.8)
        }
        var lp = pendingResolve; pendingResolve = null; lp(result);
      }
    }

    async function createDom() {
      div = document.createElement('div');
      div.style.border = '2px solid red'; div.style.padding = '10px'; div.style.width = '660px'; div.style.background = '#000';
      labelElement = document.createElement('div');
      labelElement.innerText = "SISTEMA DE SEGURAN√áA ATIVO";
      labelElement.style.color = 'white'; labelElement.style.fontWeight = 'bold';
      div.appendChild(labelElement);
      video = document.createElement('video');
      video.style.display = 'block'; video.width = 640; video.height = 480;
      div.appendChild(video);
      stream = await navigator.mediaDevices.getUserMedia({video: {width: 640, height: 480}});
      video.srcObject = stream; await video.play();
      captureCanvas = document.createElement('canvas');
      captureCanvas.width = 640; captureCanvas.height = 480;
      imgElement = document.createElement('img');
      imgElement.style.position = 'absolute'; imgElement.style.top = '40px'; imgElement.style.left = '20px';
      imgElement.style.opacity = '0.8'; div.appendChild(imgElement);
      const stopBtn = document.createElement('button');
      stopBtn.textContent = "PARAR MONITORAMENTO";
      stopBtn.onclick = () => { shutdown = true; };
      div.appendChild(stopBtn);
      document.body.appendChild(div);
      window.requestAnimationFrame(onAnimationFrame);
    }

    async function stream_frame(label, imgData) {
      if (shutdown) { removeDom(); shutdown = false; return ""; }
      if (div === null) await createDom();
      if (labelElement) labelElement.innerText = label;
      if (imgData) imgElement.src = imgData;
      return new Promise((resolve) => { pendingResolve = resolve; });
    }
    ''')
  display(js)

# --- 3. LOOP PRINCIPAL ---
# Recarrega os pesos para garantir
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
predictor = DefaultPredictor(cfg)

video_stream()
label_html = 'Iniciando C√¢mera...'
bbox_img_data = ''

try:
    while True:
        img_data = eval_js('stream_frame("{}", "{}")'.format(label_html, bbox_img_data))
        if not img_data: break

        binary = b64decode(img_data.split(',')[1])
        img = cv2.imdecode(np.frombuffer(binary, np.uint8), -1)
        outputs = predictor(img)
        instances = outputs["instances"].to("cpu")

        # Filtrar apenas classe 1 (pessoas)
        pessoas_only = instances[instances.pred_classes == 1]

        canvas = np.zeros((480, 640, 3), dtype=np.uint8)
        v = Visualizer(canvas, metadata=pessoas_metadata, scale=1.0)
        out = v.draw_instance_predictions(pessoas_only)

        bbox_img_data = 'data:image/png;base64,' + array_to_image(out.get_image())
        label_html = f"SEGURAN√áA CAMPUS: {len(pessoas_only)} PESSOA(S) DETECTADA(S)"
except Exception as e:
    print("Monitoramento finalizado.")
```

## üìÅ Estrutura do Reposit√≥rio

* `data/`: Amostras do dataset e anota√ß√µes.
* `training/`: Scripts de configura√ß√£o e treinamento (Fine-tuning).
* `inference/`: Scripts para teste em imagens e webcam.
* `results/`: Gr√°ficos de m√©tricas e evid√™ncias visuais.
* `model/`: (Link para download do modelo .pth).

---
**Autor:** Mateus Oliveira
**Disciplina:** Intelig√™ncia Artificial - 2025.2
